{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piziomo/Machine-Learning/blob/main/CNN/AI_in_10_Lines_of_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lwRRSjyfZUo"
      },
      "source": [
        "# Data Scientists Hate Him: Train A Tissue Classifier In 4 Simple Steps\n",
        "\n",
        "Image classification is a common task in machine learning and computational pathology. Image classification is predicting a label (AKA class) for a given image. E.g. is this an image of a dog or a cat?\n",
        "\n",
        "## Which of These are Cats and Which are Dogs?\n",
        "\n",
        "![Cats and dogs](https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/images/cats_vs_dogs_samples.jpg)\n",
        "(Rstudio blog)\n",
        "\n",
        "We intuitively know which of these images are of cats and which are of dogs. How would you tell a computer how to do this? Defining how to do this is hard!\n",
        "\n",
        "Instead of describing the process by hand we can use a large dataset of images with the labels/classes assigned by a human. From this dataset we can find patterns in them images which correlate with cats and patterns which correlate with dogs. In practice, this often works quite well.\n",
        "\n",
        "This process can also be used to classifying patches of a slide into classes e.g. tumour, simple stroma, background, mucosa etc.\n",
        "\n",
        "![Tissue Patches from Kather 2016](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fsrep27988/MediaObjects/41598_2016_Article_BFsrep27988_Fig1_HTML.jpg?as=webp)\n",
        "\n",
        "*(a) tumour epithelium, (b) simple stroma, (c) complex stroma (stroma that contains single tumour cells and/or single immune cells), (d) immune cell conglomerates, (e) debris and mucus, (f) mucosal glands, (g) adipose tissue, (h) background.*\n",
        "\n",
        "(Kather et al. 2016)\n",
        "\n",
        "## Hands On: Training Your Own Model\n",
        "\n",
        "Training most machine learning models involves a few main steps:\n",
        "\n",
        "1. Aquiring / preparing data.\n",
        "2. Loading data.\n",
        "3. Training a model (AKA fitting the model to the data).\n",
        "4. Evaluating the model predictions (AKA inference).\n",
        "\n",
        "\n",
        "Data is usually split into two sets: training and validation.\n",
        "\n",
        "- Training data is used to iteratively make small changes (AKA updates) to the model so that it more closely relates the data to their respective labels (AKA classes).\n",
        "- Validation is not used to update the model during training. It is used to assess the performance of the model during training.\n",
        "\n",
        "The rest of this notebook will walk you through those steps.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0: Before We Begin: Make a Copy and Enable a GPU\n",
        "\n",
        "### Make a Copy\n",
        "\n",
        "To run the code you need to make a copy of this notebook and sign into a Google account.\n",
        "\n",
        "1. Click 'Open in Playground'\n",
        "2. Click 'Copy to Drive'\n",
        "\n",
        "### Enable GPU\n",
        "\n",
        "(Optional but much faster than CPU only)!\n",
        "\n",
        "1. Click 'Runtime' from the menu above ‚òùÔ∏è\n",
        "2. Select 'Change runtime type'\n",
        "3. Under 'Hardware Accelerator' Select the 'GPU' option\n",
        "4. Congratulations! You have enabled the GPU!\n",
        "5. Run the cell below to double check üëá\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwHNH4IOgDK9"
      },
      "source": [
        "## Step 1: Download The Data (1 Minute)\n",
        "\n",
        "The following two lines will\n",
        "\n",
        "1. Download a zip of the dataset.\n",
        "2. Unzip the data.\n",
        "\n",
        "> ‚ñ∂Ô∏è Run the cell below¬π to load the data and see a short summary üëá\n",
        "\n",
        "¬π Click inside the cell then press the play button to the side or press ‚åò/Ctrl+Enter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTyPu9qBV58j"
      },
      "source": [
        "# Use wget to download the dataset\n",
        "# Short URL: https://tinyurl.com/kather-5000\n",
        "! wget https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip\n",
        "# Unzip it\n",
        "# (-o = overwrite if needed, -q = be quiet i.e. don't print out lots of unnecessary things)\n",
        "! unzip -o -q Kather_texture_2016_image_tiles_5000.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0aEVilFgGmw"
      },
      "source": [
        "## Step 2: Import The fast.ai Module & Load The Dataset (1 Second)\n",
        "\n",
        "The [fast.ai](https://www.fast.ai/) python module simplifies training a basic model by doing a lot of the 'heavy lifting' for us. By importing fast.ai we can get to training a model very quickly!\n",
        "\n",
        "The following lines will\n",
        "\n",
        "1. Import the fast.ai module.\n",
        "2. Load the dataset from a folder, setting aside 20% for validation.\n",
        "3. Print out a summary of the data.\n",
        "\n",
        "Note that `seed=123`. This is the random seed used for reproducability. We set the seed to a constant value here so that if we run the same code again, the same 'random' (pseudorandom) 20% of the dataset is used for validation.\n",
        "\n",
        "The data is loaded as $x$ and $y$ pairs. The $x$ here is an image and the $y$ is the value we want to predict from the image AKA the label.\n",
        "\n",
        "> ‚ñ∂Ô∏è Run the cell below to load the data and see a short summary üëá"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhCJ1QYUV5Hv",
        "outputId": "f06a1227-8d56-4948-b8b5-da98103ba7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "from fastai.vision import *\n",
        "data = ImageDataBunch.from_folder('Kather_texture_2016_image_tiles_5000', valid_pct=0.2, seed=123)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageDataBunch;\n",
              "\n",
              "Train: LabelList (4000 items)\n",
              "x: ImageList\n",
              "Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150)\n",
              "y: CategoryList\n",
              "03_COMPLEX,03_COMPLEX,03_COMPLEX,03_COMPLEX,03_COMPLEX\n",
              "Path: Kather_texture_2016_image_tiles_5000;\n",
              "\n",
              "Valid: LabelList (1000 items)\n",
              "x: ImageList\n",
              "Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150),Image (3, 150, 150)\n",
              "y: CategoryList\n",
              "05_DEBRIS,01_TUMOR,04_LYMPHO,04_LYMPHO,03_COMPLEX\n",
              "Path: Kather_texture_2016_image_tiles_5000;\n",
              "\n",
              "Test: None"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9zmCLMGbZk2"
      },
      "source": [
        "Above we can see that 80% (4000/5000) of our data is in the training set, and 20% (1000/5000) is in the validation set.\n",
        "\n",
        "The `x:` and `y:` is showing a preview of the first 5 pairs items in each split of the dataset.\n",
        "\n",
        "We can see that they both contain images which are 150px by 150px square with three colour channels for red, green and blue (RGB). This is show next to `x:` by `Image (3, 150, 150)`.\n",
        "\n",
        "The labels corresponding to each image are shown next to `y:` e.g. `01_TUMOR,08_EMPTY,03_COMPLEX,03_COMPLEX,02_STROMA` (validation labelds for seed 123).\n",
        "\n",
        "> ‚ñ∂Ô∏è Run the cell below to see a preview of the images and their labels üëá"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsu_6-oRd_Jf"
      },
      "source": [
        "data.show_batch(rows=4, figsize=(6,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ_TM8GCf7aI"
      },
      "source": [
        "## Step 3: Train The Model (2 Minutes)\n",
        "\n",
        "Next, we load a model (here a convolutional neural network (CNN) called ResNet) and then train on (or 'fit to') the data.\n",
        "\n",
        "The following lines will\n",
        "\n",
        "1. Load the model, in this case ResNet-18.\n",
        "2. Use accuracy as a metric to measure the model performance.¬π\n",
        "3. Tell the model to plot a graph of loss¬≤ over time using a 'callback'.¬≥\n",
        "4. Train the model, AKA fit to the data.\n",
        "\n",
        "¬πNote that fastai metrics are calculated on the validation set.\n",
        "\n",
        "¬≤Loss is a measure of the difference between what the model predicted and what it, ideally, should have predicted. This could be simply the average of the difference between the prediced and the true image labels, or it could be something more complicated e.g. to penalised larger differences more harshly.\n",
        "\n",
        "¬≥A callback is something which the training function can reference after each epoch (pass through all the training data). Here the callback is a function which can plot a graph.\n",
        "\n",
        "\n",
        "> ‚ñ∂Ô∏è Run the cell below to train the model üëá"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFKdI2GHWYrA"
      },
      "source": [
        "learn = cnn_learner(data, models.resnet18, metrics=accuracy, callback_fns=ShowGraph)\n",
        "learn.fit(5)  # Train for 5 epochs (passes through all of the training data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgR_CHR5mlOB"
      },
      "source": [
        "The table above shows a summary of the training process after each epoch with the training loss, validation loss, (validation) accuracy and time taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34j-ZKL-frs-"
      },
      "source": [
        "## Step 4: Plot A Confusion Matrix (1 Minute)\n",
        "\n",
        "The accuracy alone from the previous step tells us how well the model perfomed **on average** across all of the images. This **doesn't tell us how accurately it can predict each type of image** e.g. tumour. One way to find out more about the model's performance on each label or class is to plot a confusion matrix.\n",
        "\n",
        "> ‚ñ∂Ô∏è Run the cell below to plot a confusion matrix üëá"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ZcYKONZ3Sk"
      },
      "source": [
        "preds, y, losses = learn.get_preds(with_loss=True)\n",
        "interp = ClassificationInterpretation(learn, preds, y, losses)\n",
        "print(accuracy(preds, y)) # Same as accuracy in table from previous cell\n",
        "interp.plot_confusion_matrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9XakusukyDs"
      },
      "source": [
        "Here you can see that most of the images lie on the diagonal which is good. This means that the predictions line up well with the actual labels/classes.\n",
        "\n",
        "Note that there are a few stray high values off of the diagonal e.g. actual debris and predicted stroma. The debris and stroma images often look quite similar to each other. Therefore, it make sense that the model struggles to seperate these classes more than others.\n",
        "\n",
        "Additionally, complex and stroma appear to be sometimes confused. Does this also make sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpNpfFG4j1GZ"
      },
      "source": [
        "# Congratulations!\n",
        "\n",
        "You have trained a neural network image classifier and evaluated its preictions in just 10 lines of Python code!\n",
        "\n",
        "![Congratulations](https://media2.giphy.com/media/pAaROqrcFT5Ze/giphy.gif?cid=790b761123ced419de2444db29c2657385c36ec5944f6e96&rid=giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n88wEkEgwpo"
      },
      "source": [
        "# Extra Bits To Play With\n",
        "\n",
        "Here are some extra bits which you can play aound with.\n",
        "\n",
        "## Show An Image & Its Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWpbFuTphDGZ"
      },
      "source": [
        "n = 0\n",
        "img = learn.data.valid_ds[n][0] # Get the first (nth) image from the test set\n",
        "img.show() # Show the image\n",
        "print(learn.predict(img)[0]) # Make a prediction and display it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsmUhumnkDKs"
      },
      "source": [
        "## Show Top Losses\n",
        "\n",
        "Plot the images which resulted in the highest loss i.e. show images which the network predicted incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwFVARvggVpW"
      },
      "source": [
        "interp.plot_top_losses(9, figsize=(14, 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qUTB2c7W8Eq"
      },
      "source": [
        "# Where to Go from Here\n",
        "\n",
        "There are many resources on the web to help develop an understanding of machine learning methods. Below I have linked just a small selection of accesible resources to get started.\n",
        "\n",
        "## Videos\n",
        "\n",
        "There are many assicible video which can give a good background to machine learning and sub-topics.\n",
        "\n",
        "### Computerphile (YouTube)\n",
        "\n",
        "This is an YouTube channel run by Brady John Haran which features accesible videos on computer science topics and is the sister channel of 'Numberphile'.\n",
        "\n",
        "- [Inside a Neural Network](https://youtu.be/BFdMrDOx_CM)\n",
        "- [How Blurs & Filters Work (A core concept behind CNNs)](https://youtu.be/C_zFhWdM4ic)\n",
        "- [Neural Network that Changes Everything (CNNs)](https://youtu.be/py5byOOHZM8)\n",
        "- [Deep Learning with CNNs](https://youtu.be/TJlAxW-2nmI)\n",
        "- [Deep Learning (2017)](https://youtu.be/l42lr8AlrHk)\n",
        "- [Deep Learning (2019)](https://youtu.be/TJlAxW-2nmI)\n",
        "- [Data Analysis with Dr Mike Pound (Playlist)](https://www.youtube.com/playlist?list=PLzH6n4zXuckpfMu_4Ff8E7Z1behQks5ba)\n",
        "\n",
        "### CGP Grey (YouTube)\n",
        "\n",
        "- [How Machines Learn](https://youtu.be/R9OHn5ZF4Uo)\n",
        "\n",
        "## Online Interactive Demos\n",
        "\n",
        "- [A Neural Network Playground - Tensorflow](https://playground.tensorflow.org/)\n",
        "- [Distill - An Interactive Online Journal](https://distill.pub/)\n",
        "\n",
        "## Glossaries\n",
        "\n",
        "- [Machine learning glossary of important terms - Microsoft](https://docs.microsoft.com/en-us/dotnet/machine-learning/resources/glossary)\n",
        "- [Glossary of Machine Learning Terms - Semantic Bits](https://semanti.ca/blog/?glossary-of-machine-learning-terms)\n",
        "\n",
        "\n",
        "## Books\n",
        "\n",
        "- Hello World: How to be Human in the Age of the Machine by Hannah Fry, Associate Professor in mathematics at University College London.\n",
        "  - A tour of ethical implications which arise when trusting machines to make decisions for us. Topics include crime and punishment, medical interventions, and self driving cars."
      ]
    }
  ]
}